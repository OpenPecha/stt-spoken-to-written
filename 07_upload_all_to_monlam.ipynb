{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download all speech data from s3 before running inference\n",
    "!aws s3 sync s3://monlam.ai.stt/wav16k/ ./wav16k/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers tqdm\n",
    "# Install PyTorch (Choose the appropriate command for your setup from the PyTorch website)\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# Or install TensorFlow\n",
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch (Choose the appropriate command for your setup from the PyTorch website)\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# Or install TensorFlow\n",
    "!pip install tensorflow\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('04_combine_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path'] = df['url'].apply(lambda x: '/'.join(x.split('/')[-2:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['inf'] = \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('latest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline\n",
    "from pathlib import Path\n",
    "\n",
    "generator = pipeline(model=\"spsither/mms_300_v3.1020\")\n",
    "\n",
    "predictions = []\n",
    "\n",
    "for path in tqdm(df['path']):\n",
    "    path = Path(path)\n",
    "    if not path.exists(): \n",
    "        predictions.append('')\n",
    "        continue\n",
    "    opt = generator(str(path))\n",
    "    predictions.append(opt[\"text\"])\n",
    "\n",
    "df['inf'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import pipeline\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('latest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe from the latest checkpoint\n",
    "df = pd.read_csv(\"latest.csv\")\n",
    "\n",
    "# Initialize the pipeline\n",
    "generator = pipeline(model=\"spsither/mms_300_v3.1020\")\n",
    "\n",
    "# Filter the dataframe to only include rows where the 'inf' column is empty\n",
    "df_to_process = df[df['inf'] == \" \"]\n",
    "\n",
    "# Counter to keep track of rows processed\n",
    "counter = 0\n",
    "\n",
    "# Run inference on the filtered dataframe\n",
    "for index, path in tqdm(df_to_process['path'].items(), desc=\"Processing Paths\", total=len(df_to_process)):\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        df.at[index, 'inf'] = ' '\n",
    "    else:\n",
    "        opt = generator(str(path))\n",
    "        df.at[index, 'inf'] = opt[\"text\"]\n",
    "    \n",
    "    # Increment the counter\n",
    "    counter += 1\n",
    "\n",
    "    # Save the dataframe at every 100 intervals\n",
    "    if counter % 100 == 0:\n",
    "        df.to_csv(\"latest.csv\", index=False)\n",
    "        print(f\"Saved progress at {counter} rows.\")\n",
    "\n",
    "# Save the final updated dataframe\n",
    "df.to_csv(\"latest.csv\", index=False)\n",
    "print(\"Final save completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('04_combine_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths and bucket details\n",
    "# save combine with time stamp to s3 \n",
    "local_file = '04_combine_all.csv'\n",
    "bucket_name = 'monlam.ai.stt'\n",
    "s3_file_name = 'tsv/04_combine_all_04_jun_2024.csv'\n",
    "\n",
    "# Run the AWS CLI command to upload the file\n",
    "!aws s3 cp {local_file} s3://{bucket_name}/{s3_file_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def split_csv(file_path, rows_per_file, output_dir):\n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Read the input CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "\n",
    "    # Get the total number of rows in the DataFrame\n",
    "    total_rows = len(df)\n",
    "\n",
    "    # Calculate the number of files needed\n",
    "    num_files = (total_rows // rows_per_file) + (1 if total_rows % rows_per_file != 0 else 0)\n",
    "\n",
    "    # Split the DataFrame and save each part as a new CSV file\n",
    "    for i in range(num_files):\n",
    "        start_row = i * rows_per_file\n",
    "        end_row = start_row + rows_per_file\n",
    "        part_df = df.iloc[start_row:end_row]\n",
    "        folder_dir = os.path.join(output_dir, f'sw{str(i+1).zfill(5)}')\n",
    "        if not os.path.exists(folder_dir):\n",
    "            os.makedirs(folder_dir)\n",
    "        output_file = os.path.join(folder_dir, f'sw{str(i+1).zfill(5)}.csv')\n",
    "        part_df = part_df.loc[:, ~part_df.columns.str.contains('^Unnamed')]\n",
    "        part_df.to_csv(output_file, index=False)\n",
    "    print(f'Successfully split {file_path} into {num_files} files.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the input file path and the number of rows per file\n",
    "input_file_path = '04_combine_all.csv'\n",
    "rows_per_file = 1000\n",
    "output_dir = 'output'\n",
    "# Split the CSV file\n",
    "split_csv(input_file_path, rows_per_file, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyGithub in /opt/conda/lib/python3.10/site-packages (2.3.0)\n",
      "Requirement already satisfied: pynacl>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub) (1.5.0)\n",
      "Requirement already satisfied: requests>=2.14.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub) (2.31.0)\n",
      "Requirement already satisfied: pyjwt>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub) (4.9.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from PyGithub) (2.1.0)\n",
      "Requirement already satisfied: Deprecated in /opt/conda/lib/python3.10/site-packages (from PyGithub) (1.2.14)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (42.0.2)\n",
      "Requirement already satisfied: cffi>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from pynacl>=1.4.0->PyGithub) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.14.0->PyGithub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.14.0->PyGithub) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.14.0->PyGithub) (2023.11.17)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from Deprecated->PyGithub) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install PyGithub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from github import Github\n",
    "import subprocess\n",
    "\n",
    "def repo_exists(org, repo_name):\n",
    "    \"\"\"Check if a repository exists in the organization.\"\"\"\n",
    "    try:\n",
    "        repo = org.get_repo(repo_name)\n",
    "        return repo.clone_url\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def create_github_repo(repo_name, org, token):\n",
    "    # Authenticate to GitHub\n",
    "    g = Github(token)\n",
    "    \n",
    "    # Get the organization\n",
    "    org = g.get_organization(org)\n",
    "    \n",
    "    # Check if the repository already exists\n",
    "    repo_url = repo_exists(org, repo_name)\n",
    "    if repo_url:\n",
    "        print(f'Repository {repo_name} already exists.')\n",
    "        return repo_url\n",
    "    \n",
    "    # Create the repository\n",
    "    try:\n",
    "        repo = org.create_repo(name=repo_name)\n",
    "        print(f'Successfully created repository: {repo_name}')\n",
    "        return repo.clone_url\n",
    "    except Exception as e:\n",
    "        print(f'Failed to create repository: {repo_name}')\n",
    "        print(f'Error: {e}')\n",
    "        return None\n",
    "\n",
    "def upload_files_to_repo(local_dir, repo_url, token):\n",
    "    # Set Git configuration for user\n",
    "    subprocess.run(['git', 'config', '--global', 'user.email', 'ganga@esukhia.org'])\n",
    "    subprocess.run(['git', 'config', '--global', 'user.name', 'gangagyatso4364'])\n",
    "    \n",
    "    # Initialize a local git repository if it does not exist\n",
    "    if not os.path.exists(os.path.join(local_dir, '.git')):\n",
    "        subprocess.run(['git', 'init'], cwd=local_dir)\n",
    "    \n",
    "    # Add the remote origin, ignoring error if it already exists\n",
    "    subprocess.run(['git', 'remote', 'remove', 'origin'], cwd=local_dir)\n",
    "    subprocess.run(['git', 'remote', 'add', 'origin', repo_url], cwd=local_dir)\n",
    "    \n",
    "    # Pull from the remote repository to get the latest changes\n",
    "    subprocess.run(['git', 'pull', 'origin', 'master'], cwd=local_dir)\n",
    "    \n",
    "    # Add all files to the staging area\n",
    "    subprocess.run(['git', 'add', '.'], cwd=local_dir)\n",
    "    \n",
    "    # Commit the files\n",
    "    subprocess.run(['git', 'commit', '-m', 'Initial commit'], cwd=local_dir)\n",
    "    \n",
    "    # Push the files to the GitHub repository\n",
    "    subprocess.run(['git', 'push', 'origin', 'master'], cwd=local_dir)\n",
    "\n",
    "def create_repos_for_folders(output_dir, org_name, token):\n",
    "    for folder_name in os.listdir(output_dir):\n",
    "        folder_path = os.path.join(output_dir, folder_name)\n",
    "        if os.path.isdir(folder_path):\n",
    "            repo_url = create_github_repo(folder_name, org_name, token)\n",
    "            if repo_url:\n",
    "                upload_files_to_repo(folder_path, repo_url.replace('https://', f'https://{token}@'), token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created repository: .ipynb_checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: couldn't find remote ref master\n",
      "error: src refspec master does not match any\n",
      "error: failed to push some refs to 'https://github.com/MonlamAI/.ipynb_checkpoints.git'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "\n",
      "Initial commit\n",
      "\n",
      "nothing to commit (create/copy files and use \"git add\" to track)\n",
      "Successfully created repository: sw00002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: couldn't find remote ref master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master 791ef8a] Initial commit\n",
      " 1 file changed, 1001 insertions(+)\n",
      " create mode 100644 .ipynb_checkpoints/sw00002-checkpoint.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/MonlamAI/sw00002.git\n",
      " * [new branch]      master -> master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created repository: sw00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: couldn't find remote ref master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch master\n",
      "nothing to commit, working tree clean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/MonlamAI/sw00001.git\n",
      " * [new branch]      master -> master\n"
     ]
    }
   ],
   "source": [
    "# Specify your GitHub organization name and personal access token\n",
    "org_name = 'MonlamAI'\n",
    "token = 'your_token'  # Replace with your actual token\n",
    "\n",
    "# Output directory containing the folders\n",
    "output_dir = 'output_repo'\n",
    "\n",
    "# Create repositories for each folder in the output directory\n",
    "create_repos_for_folders(output_dir, org_name, token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
